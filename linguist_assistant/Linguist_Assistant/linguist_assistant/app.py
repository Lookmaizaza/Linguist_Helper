import re
import time
import html
import concurrent.futures
from io import BytesIO
import pandas as pd
import requests
import streamlit as st
from bs4 import BeautifulSoup
import os
import importlib

# Suppress TensorFlow warnings
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

# --- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Library ---
try:
    import trafilatura
    TRAFILATURA_AVAILABLE = True
except ImportError:
    TRAFILATURA_AVAILABLE = False

try:
    import deepcut
    DEEPCUT_AVAILABLE = True
except ImportError:
    DEEPCUT_AVAILABLE = False

def tokenize_text(text: str):
    """Tokenize text using deepcut with robust session recovery"""
    if not DEEPCUT_AVAILABLE or not text:
        return text.split() if text else []
    
    try:
        from deepcut import tokenize
        return tokenize(text)
    except Exception as e1:
        # Session corrupted - reload deepcut module completely
        try:
            import tensorflow as tf
            tf.keras.backend.clear_session()
            
            # Reload deepcut to reinitialize the tokenizer
            import deepcut as dc
            importlib.reload(dc)
            from deepcut import tokenize
            return tokenize(text)
        except Exception as e2:
            # Ultimate fallback to word split
            return text.split()

# --- Pre-compile Regex & Junk ---
RE_CLEAN = re.compile(r"[\u200B-\u200D\uFEFF]")
RE_KEEP = re.compile(r"[^a-zA-Z‡∏Å-‡∏Æ‡∏∞-‡πå0-9\.\s]")
JUNK_KEYWORDS = {"‡∏´‡∏ß‡∏¢", "‡∏î‡∏ß‡∏á", "‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤", "‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠", "‡∏Ñ‡∏•‡∏¥‡∏Å", "‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å", "‡πÄ‡∏°‡∏ô‡∏π", "Login"}

def clean_text_final(text: str) -> str:
    if not text: return ""
    lines = []
    for ln in text.splitlines():
        ln = ln.strip()
        # ‡∏Å‡∏£‡∏≠‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏Ç‡∏¢‡∏∞‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô
        if len(ln) < 40 and any(j in ln for j in JUNK_KEYWORDS): continue
        if ln: lines.append(ln)
    
    text = "\n".join(lines)
    text = html.unescape(text)
    text = RE_CLEAN.sub("", text)
    text = RE_KEEP.sub(" ", text)
    return re.sub(r"\s+", " ", text).strip()

def get_content_universal(url: str):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
    try:
        # ‡πÇ‡∏´‡∏•‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö
        res = requests.get(url, headers=headers, timeout=12)
        res.raise_for_status()
        res.encoding = res.apparent_encoding
        soup = BeautifulSoup(res.content, 'html.parser')

        # --- ‡πÅ‡∏ú‡∏ô 1: Sniper Mode (‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á ID/Class) ---
        # ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏¢‡∏≠‡∏î‡∏Æ‡∏¥‡∏ï‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡πÑ‡∏ó‡∏¢/‡πÄ‡∏ó‡∏®
        targets = [
            {'class': 'EntryReaderInner'}, {'id': 'EntryReader_0'}, # Sanook
            {'class': 'article-body'}, {'itemprop': 'articleBody'}, # Standard Semantic
            {'class': 'content-detail'}, {'class': 'news-detail'},  # General Thai News
            {'class': 'story-body'}, {'role': 'main'}               # BBC / Others
        ]
        
        for attrs in targets:
            # ‡∏´‡∏≤ div ‡∏´‡∏£‡∏∑‡∏≠ article ‡∏ó‡∏µ‡πà‡∏°‡∏µ attributes ‡∏ï‡∏≤‡∏°‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
            node = soup.find(['div', 'article', 'section'], attrs)
            if node:
                # ‡∏•‡∏ö‡∏Ç‡∏¢‡∏∞‡πÉ‡∏ô‡∏Å‡πâ‡∏≠‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô ‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤‡∏Ñ‡∏±‡πà‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î)
                for junk in node(['script', 'style', 'div.ads', 'div.related']):
                    junk.decompose()
                return clean_text_final(node.get_text(separator="\n")), "Sniper (Targeted)"

        # --- ‡πÅ‡∏ú‡∏ô 2: AI Mode (Trafilatura) ---
        # ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÉ‡∏´‡πâ Trafilatura ‡∏ä‡πà‡∏ß‡∏¢‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
        if TRAFILATURA_AVAILABLE:
            # favor_precision=True ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡∏î‡∏∂‡∏á‡πÄ‡∏°‡∏ô‡∏π‡∏ï‡∏¥‡∏î‡∏°‡∏≤
            extracted = trafilatura.extract(res.content, include_comments=False, favor_precision=True)
            if extracted:
                return clean_text_final(extracted), "AI (Trafilatura)"

        # --- ‡πÅ‡∏ú‡∏ô 3: Sweep Mode (‡∏Å‡∏ß‡∏≤‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á) ---
        # ‡∏•‡∏ö‡πÅ‡∏ó‡πá‡∏Å‡∏Ç‡∏¢‡∏∞‡∏≠‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡∏´‡∏°‡∏î‡∏Å‡πà‡∏≠‡∏ô
        for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'button', 'form', 'iframe']):
            tag.decompose()
        
        # ‡∏î‡∏∂‡∏á text ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏≠‡∏¢‡∏π‡πà
        raw_text = soup.get_text(separator="\n")
        return clean_text_final(raw_text), "Fallback (Sweep)"

    except Exception as e:
        return "", f"Error: {str(e)}"

# --- UI Logic ---
st.set_page_config(page_title="Linguist Pro: Universal", layout="wide")
st.title("üåê The Linguist's Assistant (Universal Mode)")

# Initialize session state for accumulating results
if "all_results" not in st.session_state:
    st.session_state.all_results = []  # List of analyses
if "show_results" not in st.session_state:
    st.session_state.show_results = False

# Top buttons for navigation
col1, col2, col3 = st.columns([1, 1, 8])
with col1:
    if st.button("üìù ‡∏õ‡πâ‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", use_container_width=True):
        st.session_state.show_results = False
        st.rerun()
with col2:
    if len(st.session_state.all_results) > 0:
        if st.button("üìä ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", use_container_width=True):
            st.session_state.show_results = True
            st.rerun()

st.divider()

# PAGE 1: INPUT
if not st.session_state.show_results:
    st.markdown("### ‡∏õ‡πâ‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå")
    
    # Toggle between URL and Text input
    input_type = st.radio("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:", ["üîó URL", "üìù ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"], horizontal=True)
    
    if input_type == "üîó URL":
        user_input = st.text_area("‡∏ß‡∏≤‡∏á URL (‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà):", height=150, placeholder="https://example.com", key="url_input")
    else:
        user_input = st.text_area("‡∏ß‡∏≤‡∏á ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà):", height=150, placeholder="‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå", key="text_input")
    
    col_analyze, col_clear = st.columns([5, 1])
    
    with col_analyze:
        if st.button("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå", type="primary", use_container_width=True):
            items = [ln.strip() for ln in user_input.splitlines() if ln.strip()]
            if not items:
                st.warning("‚ö†Ô∏è ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏õ‡πâ‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
                st.stop()
        
            results = []
            
            if input_type == "üîó URL":
                # Process URLs
                with st.status("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏∞‡∏ö‡∏ö 3-Layer Safety Net...", expanded=True) as status:
                    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
                        future_to_url = {executor.submit(get_content_universal, url): url for url in items}
                        for future in concurrent.futures.as_completed(future_to_url):
                            url = future_to_url[future]
                            content, method = future.result()
                            
                            if content:
                                status.write(f"‚úÖ {url} -> ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ: {method}")
                                tokens = tokenize_text(content)
                                results.append({"Source": url, "Raw": content, "Tokens": tokens, "Method": method})
                            else:
                                status.write(f"‚ùå {url} -> ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤")
            else:
                # Process text input
                with st.status("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°...", expanded=True) as status:
                    for idx, text in enumerate(items, 1):
                        cleaned_text = clean_text_final(text)
                        if cleaned_text:
                            status.write(f"‚úÖ ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° {idx} -> ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
                            tokens = tokenize_text(cleaned_text)
                            results.append({"Source": f"‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° {idx}", "Raw": cleaned_text, "Tokens": tokens, "Method": "Direct Text"})
                        else:
                            status.write(f"‚ùå ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° {idx} -> ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤")
            
            if results:
                # Add to accumulated results with timestamp
                import datetime
                analysis_entry = {
                    "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "data": results
                }
                st.session_state.all_results.append(analysis_entry)
                st.session_state.show_results = True
                st.session_state.current_result_idx = len(st.session_state.all_results) - 1
                st.success(f"‚úÖ ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡πÄ‡∏û‡∏¥‡πà‡∏° {len(results)} URL")
                st.info(f"üìä ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(st.session_state.all_results)}")
                st.rerun()
            else:
                st.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏î ‡πÜ")
    
    with col_clear:
        if st.button("üóëÔ∏è", help="‡∏•‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", use_container_width=True):
            st.session_state.all_results = []
            st.success("‚úÖ ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡πâ‡∏ß")
            st.rerun()
    
    # Show summary of all analyses
    if len(st.session_state.all_results) > 0:
        st.divider()
        st.subheader("üìã ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå")
        for idx, analysis in enumerate(st.session_state.all_results, 1):
            col1, col2 = st.columns([4, 1])
            with col1:
                st.write(f"**{idx}. {analysis['timestamp']}** - {len(analysis['data'])} URL")
            with col2:
                if st.button("‡∏î‡∏π‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå", key=f"view_{idx}"):
                    st.session_state.show_results = True
                    st.session_state.current_result_idx = idx - 1
                    st.rerun()

# PAGE 2: RESULTS
else:
    if len(st.session_state.all_results) == 0:
        st.info("üì≠ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
    else:
        # Select which analysis to view
        if "current_result_idx" not in st.session_state:
            st.session_state.current_result_idx = len(st.session_state.all_results) - 1
        
        col1, col2 = st.columns([1, 5])
        with col1:
            result_options = [f"#{idx + 1} - {analysis['timestamp']}" 
                            for idx, analysis in enumerate(st.session_state.all_results)]
            selected_idx = st.selectbox("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå:", 
                                       range(len(st.session_state.all_results)),
                                       format_func=lambda x: result_options[x])
            st.session_state.current_result_idx = selected_idx
        
        results = st.session_state.all_results[st.session_state.current_result_idx]["data"]
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame
        df = pd.DataFrame(results)
        
        # Summary Sheet
        df_sum = df[["Source", "Method", "Raw"]].copy()
        df_sum["Word Count"] = df["Tokens"].apply(len)
        df_sum["Tokenized"] = df["Tokens"].apply(lambda x: "|".join(x))
        
        # Word List Sheet
        df_words = df[["Source", "Tokens"]].explode("Tokens").rename(columns={"Tokens": "Word"})
        df_words["Index"] = df_words.groupby("Source").cumcount() + 1
        
        # ===== Enhanced Results Display =====
        st.success("‚úÖ ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå")
        
        # Metrics Row
        cols = st.columns(4)
        with cols[0]:
            st.metric("üìÑ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô URL", len(results))
        with cols[1]:
            total_words = df_sum["Word Count"].sum()
            st.metric("üìù ‡∏£‡∏ß‡∏°‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå", int(total_words))
        with cols[2]:
            avg_words = df_sum["Word Count"].mean()
            st.metric("üìä ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢", f"{avg_words:.0f}")
        with cols[3]:
            st.metric("‚öôÔ∏è ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ", df_sum["Method"].mode()[0] if len(df_sum) > 0 else "N/A")
        
        st.divider()
        
        # Tabs for different views
        tab1, tab2, tab3 = st.tabs(["üìã ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•", "üî§ ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥", "üì• ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î"])
        
        with tab1:
            st.subheader("‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ï‡πà‡∏•‡∏∞ URL")
            for idx, row in df_sum.iterrows():
                with st.expander(f"üîó {row['Source']}", expanded=(idx == 0)):
                    col1, col2 = st.columns([2, 1])
                    with col1:
                        st.write("**‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö:**")
                        st.write(row['Raw'][:500] + ("..." if len(row['Raw']) > 500 else ""))
                    with col2:
                        st.write("**‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥:**")
                        st.info(f"üéØ ‡∏ß‡∏¥‡∏ò‡∏µ: {row['Method']}\n\nüìä ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå: {row['Word Count']}")
                    
                    st.write("**‡πÇ‡∏ó‡πÄ‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡πÅ‡∏•‡πâ‡∏ß:**")
                    tokens = df.iloc[idx]['Tokens']
                    # Display tokens in nice pills
                    token_html = " ".join([f"<span style='background-color: #E8F4F8; padding: 5px 10px; margin: 3px; border-radius: 15px; display: inline-block; font-size: 12px;'>{token}</span>" for token in tokens[:50]])
                    st.markdown(f"<div>{token_html}</div>", unsafe_allow_html=True)
                    if len(tokens) > 50:
                        st.caption(f"... ‡πÅ‡∏•‡∏∞‡∏≠‡∏µ‡∏Å {len(tokens) - 50} ‡∏Ñ‡∏≥")
        
        with tab2:
            st.subheader("‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
            # Count word frequency
            word_freq = df_words['Word'].value_counts().head(30)
            col1, col2 = st.columns([1, 1])
            
            with col1:
                st.write("**30 ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î:**")
                for word, count in word_freq.items():
                    st.text(f"‚Ä¢ {word} ({count})")
            
            with col2:
                st.bar_chart(word_freq)
            
            st.dataframe(df_words[["Source", "Word", "Index"]], use_container_width=True)
        
        with tab3:
            st.subheader("‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå")
            col1, col2 = st.columns(2)
            
            with col1:
                # Export to Excel
                output = BytesIO()
                with pd.ExcelWriter(output, engine='openpyxl') as writer:
                    df_sum.to_excel(writer, index=False, sheet_name='Summary')
                    df_words.to_excel(writer, index=False, sheet_name='Detailed_Words')
                
                st.download_button(
                    label="üì• ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Excel (.xlsx)",
                    data=output.getvalue(),
                    file_name=f"linguist_analysis_{st.session_state.all_results[st.session_state.current_result_idx]['timestamp'].replace(':', '-')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    use_container_width=True
                )
            
            with col2:
                # Export to CSV
                csv = df_words.to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="üìÑ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î CSV (.csv)",
                    data=csv,
                    file_name=f"linguist_analysis_{st.session_state.all_results[st.session_state.current_result_idx]['timestamp'].replace(':', '-')}.csv",
                    mime="text/csv",
                    use_container_width=True
                )